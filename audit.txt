================================================================================
  TINYLORA IMPLEMENTATION AUDIT
  Repo:  tinylora (public research pack)
  Paper: "Learning to Reason in 13 Parameters"
         John X. Morris, Niloofar Mireshghallah, Mark Ibrahim, Saeed Mahloujifar
         arXiv:2602.04118, February 2026
  Date:  2026-02-20
================================================================================

TABLE OF CONTENTS
  1. Executive Summary
  2. Core Formula Verification
  3. Forward Pass Correctness
  4. Budget Planner Verification
  5. Tie-Sharing / Weight-Sharing
  6. GRPO Training Loop Fidelity
  7. SFT Training Loop
  8. Hyperparameter Alignment
  9. Evaluation / Reward
  10. Bugs Found
  11. Deviations from Paper (Acknowledged)
  12. Deviations from Paper (Unacknowledged)
  13. Minor Concerns
  14. What Is Correct
  15. Recommendations

================================================================================
1. EXECUTIVE SUMMARY
================================================================================

The core TinyLoRA parameterization is implemented CORRECTLY. The mathematical
formulation in adapter.py faithfully reproduces the paper's Equation (Section
4.0.2). The budget planner, tie-sharing, SVD decomposition, merge/unmerge, and
forward pass are all sound.

However, the GRPO training loop is a significant simplification of the actual
GRPO algorithm (Shao et al., 2024) used in the paper, and there are several
hyperparameter/infrastructure mismatches that would prevent exact reproduction
of the paper's headline numbers. One concrete bug (wrong PDF link in README)
was found.

Severity breakdown:
  - 0 BUGS (README link was fixed during this audit)
  - 2 SIGNIFICANT deviations (GRPO simplification, no vLLM/VERL)
  - 3 MODERATE deviations (alpha scaling, no MATH format, no full config)
  - 4 MINOR concerns (initialization, LR schedule, precision, projection dist.)

================================================================================
2. CORE FORMULA VERIFICATION
================================================================================

Paper (Section 4.0.2):

    W' = W + U Sigma (sum_{i=1}^{u} v_i P_i) V^T

Where:
  - U, Sigma, V from truncated SVD of frozen W
  - P_i in R^{r x r} are fixed random matrices
  - v in R^u is the trainable coefficient vector
  - u = projection_dim, r = frozen_rank

Code (adapter.py:216-219):

    def delta_weight(self):
        m = self._mix_matrix()           # sum(v_i * P_i) -> [r, r]
        us = self.U * self.S.unsqueeze(0) # U * diag(Sigma) -> [out, r]
        return (us @ m @ self.V.transpose(0, 1)).to(self.weight.dtype)

Expanding: delta = U*diag(S) @ M @ V^T  where M = sum(v_i * P_i)

This is exactly: U Sigma (sum v_i P_i) V^T.  CORRECT.

SVD computation (adapter.py:180-187):
  - Weight cast to float32 for SVD stability
  - U: [out_features, r], S: [r], V: [in_features, r]
  - Uses torch.linalg.svd with full_matrices=False
  - Rank clamped: min(frozen_rank, in_features, out_features)

All SVD factor shapes and orientations match the paper.  CORRECT.

================================================================================
3. FORWARD PASS CORRECTNESS
================================================================================

The forward pass (adapter.py:235-247) avoids materializing the full delta
weight matrix by computing x @ delta_W^T efficiently:

    x_flat @ V @ M^T @ diag(S) @ U^T

Verification:
  delta_W = U diag(S) M V^T
  x @ delta_W^T = x @ V M^T diag(S) U^T

Code:
  1. proj = x_flat @ V         -> x V          [n, r]
  2. proj = proj @ m.T          -> x V M^T      [n, r]
  3. proj = proj * S            -> x V M^T S    [n, r]  (element-wise = diag)
  4. delta = proj @ U^T         -> x V M^T S U^T [n, out]

This matches.  CORRECT.

Float32 upcasting for numerical stability (line 240):  GOOD PRACTICE.

The result is cast back to base dtype before adding to base output (line 246).
Merge path (line 225) and forward path produce equivalent results.  CORRECT.

================================================================================
4. BUDGET PLANNER VERIFICATION
================================================================================

Paper: For Qwen2.5-7B-Instruct with 32 layers x 7 modules = 224 target
modules, training with 13 total parameters.

Code (adapter.py:106-143):
  estimate_trainable_params(N, u, tie) = ceil(N / tie) * u
  choose_tie_factor_for_budget() sweeps tie in [1, N] to minimize |actual - target|
  choose_budget_plan() additionally sweeps projection_dim candidates

Manual verification for budget=13, num_modules=224, projection_dim=1:
  tie_factor=18: ceil(224/18) = ceil(12.44) = 13 groups * 1 = 13 params  EXACT
  tie_factor=17: ceil(224/17) = ceil(13.17) = 14 params (too many)
  tie_factor=19: ceil(224/19) = ceil(11.79) = 12 params (too few)

The planner correctly selects tie_factor=18 for budget=13.  CORRECT.

Test coverage: test_adapter.py:58-64 verifies this with both 13 and 49 budgets.

================================================================================
5. TIE-SHARING / WEIGHT-SHARING
================================================================================

Paper (Section 4.0.2): "We define the weight tying factor n_tie as the number
of modules sharing a single v... With full weight tying (n_tie = nm), all
modules share a single v."

Paper mentions two strategies (Section 6.5):
  - "structured": same-type modules share (e.g., all q_proj together)
  - "tiled": nearby modules share regardless of type

Code implements four modes: "none", "all", "tiled", "structured".

Tiled (adapter.py:84-86): Sorts modules by layer index then name, chunks by
tie_factor.  CORRECT - matches paper's "nearby modules share regardless of type."

Structured (adapter.py:88-101): Groups by module suffix (q_proj, k_proj, etc.),
then chunks within each group.  CORRECT - matches paper's "same type share."

Parameter sharing mechanics: Shared nn.Parameter objects are passed to
TinyLoRALinear as shared_v. PyTorch's parameters() deduplicates by id().
CORRECT.

Deterministic seeding per group (adapter.py:326):
  seed = config.seed + (group_id * 9973)
Per-module P matrix seeding (adapter.py:339-340):
  SHA256 hash of module name -> deterministic, unique P per module.
CORRECT - ensures reproducibility while giving each module distinct projections.

================================================================================
6. GRPO TRAINING LOOP FIDELITY
================================================================================

*** THIS IS THE MOST SIGNIFICANT DEVIATION ***

Paper states (Section 5.1): "reinforcement learning, in particular Group
Relative Policy Optimization (GRPO) (Shao et al., 2024)"

Paper also states (Section 5.1): "We run all RL experiments within the
open-source VERL framework (Sheng et al., 2024), using vLLM (Kwon et al.,
2023) for inference."

The actual GRPO algorithm (DeepSeekMath, Shao et al. 2024) includes:
  a) Per-token log-probability ratios (pi_theta / pi_ref)
  b) PPO-style clipping of the ratio
  c) Per-token advantage weighting
  d) Optional KL divergence penalty
  e) Importance sampling between old and new policy

Code (train_grpo_tinylora.py:278-324) implements:
  a) Sum of log-probs for generated tokens (not per-token ratios)
  b) No clipping
  c) Group-level advantage: reward - mean(group_rewards)
  d) No KL penalty (matches paper for GSM8K, but structurally absent)
  e) No importance sampling ratio

Loss formula in code:
    loss = -(advantages * sum_logprobs).mean()

This is vanilla REINFORCE with a group-relative baseline, NOT full GRPO.

IMPACT: For the 13-parameter regime, the model changes so little from the base
that importance sampling ratios would be ~1.0 and clipping would never
activate. So the simplification is likely harmless for reproducing the
headline result, but becomes more problematic at larger parameter budgets.

The AUDIT.md does acknowledge: "The RL script is GRPO-style, not a claim of
bit-exact reproduction of any external training stack." This is honest but the
gap is substantial.

VERDICT: SIGNIFICANT DEVIATION, partially mitigated by tiny parameter regime.

================================================================================
7. SFT TRAINING LOOP
================================================================================

Code (train_sft_tinylora.py:76-101):
  - Prompt tokens correctly masked with labels=-100
  - Uses model's native cross-entropy loss via labels argument
  - Padding handled correctly (pad_token_id, attention_mask)
  - Batch built with proper truncation to max_length

Standard practice, correctly implemented.  CORRECT.

================================================================================
8. HYPERPARAMETER ALIGNMENT
================================================================================

Paper GSM8K settings:
  - Full dataset (~7,500 training examples)
  - 3 epochs
  - 4 samples per problem (group_size)
  - Batch size 64 (across 4 GPUs)
  - Max generation length 4096
  - Temperature 1.0
  - No KL penalty
  - LR sweep: {1e-7, 5e-7, 1e-6, 5e-6, 1e-5, 1e-4, 2e-4}
  - Frozen rank r=2
  - Exact-match reward

Code argparse defaults:
  - epochs=3                        MATCH
  - group-size=4                    MATCH
  - temperature=1.0                 MATCH
  - frozen-rank=2                   MATCH
  - batch-prompts=2                 MISMATCH (paper: 64)
  - max-new-tokens=256              MISMATCH (paper: 4096)
  - train-limit=0 (unlimited)       MATCH (when not set)
  - eval-limit=256                  MISMATCH (paper: full test set)

Fast config (configs/replication_13p_fast.yaml):
  - epochs: 1                       MISMATCH (paper: 3)
  - batch_prompts: 2                MISMATCH (paper: 64)
  - group_size: 2                   MISMATCH (paper: 4)
  - train_limit: 64                 MISMATCH (paper: full)
  - eval_limit: 64                  MISMATCH (paper: full test)
  - max_prompt_tokens: 512          MISMATCH (paper: 1024)
  - max_new_tokens: 128             MISMATCH (paper: 4096)
  - LRs: match paper                MATCH

The fast config is clearly for development/testing, not reproduction. However,
there is NO provided config that matches the paper's exact settings.

VERDICT: MODERATE DEVIATION. Defaults partially match; no full-fidelity config.

================================================================================
9. EVALUATION / REWARD
================================================================================

Paper: "All our RL experiments use exact-match reward."
Paper evaluates on: GSM8K, MATH500, Minerva Math, OlympiadBench, AIME24, AMC23

Code reward.py:
  - BOXED_RE matches `#### <answer>` format (GSM8K convention)
  - NUMBER_RE provides fallback to last number in text
  - normalize_answer strips commas and trailing periods
  - exact_match_reward returns 1.0 or 0.0

This correctly handles GSM8K's answer format.  CORRECT for GSM8K.

MISSING: The code does NOT handle \boxed{...} format used by MATH, AIME, AMC,
and other benchmarks tested in the paper. To reproduce the full Table 2 results,
a \boxed{} parser would be needed.

VERDICT: MODERATE DEVIATION for multi-benchmark reproduction.

================================================================================
10. BUGS FOUND
================================================================================

No bugs found. (Previously identified wrong PDF link in README.md was fixed.)

================================================================================
11. DEVIATIONS FROM PAPER (ACKNOWLEDGED IN AUDIT.md)
================================================================================

D1. GRPO simplification (AUDIT.md Section 2)
    - Acknowledged as "GRPO-style, not bit-exact reproduction"
    - See Section 6 above for detailed analysis

D2. Target modules hardcoded to Qwen/LLaMA convention
    - q_proj, k_proj, v_proj, o_proj, up_proj, down_proj, gate_proj
    - Paper also uses these 7 modules per layer
    - CORRECT for supported models; would need changes for others

D3. Not using vLLM/VERL framework
    - Paper explicitly uses VERL + vLLM
    - Code uses HuggingFace transformers + standard generation
    - Paper mentions merging weights for vLLM compatibility and using
      truncated importance sampling (Ionides 2008; Yao et al. 2025) to handle
      the training/inference mismatch - this is NOT implemented

================================================================================
12. DEVIATIONS FROM PAPER (UNACKNOWLEDGED)
================================================================================

D4. ALPHA/SCALE FACTOR
    Paper formula: W' = W + U Sigma (sum v_i P_i) V^T
    Code: W' = W + [U Sigma (sum v_i P_i) V^T] * (alpha / projection_dim)

    Default alpha=1.0. For projection_dim=1: scale=1.0 (matches paper).
    For projection_dim=2: scale=0.5 (NOT in paper formula).

    This is borrowed from standard LoRA practice (alpha/r scaling) and is
    reasonable engineering, but it changes the optimization landscape for
    projection_dim > 1 compared to a literal reading of the paper.

    Impact: None for the 13-parameter headline result (projection_dim=1).
    Potentially affects results at other budget levels.

D5. TRAINABLE VECTOR INITIALIZATION
    Paper: Does not specify initialization of v.
    Code: inject_tinylora() initializes v = zeros + 1e-3 * randn (line 332).
    Code: TinyLoRALinear.__init__() initializes v = zeros (line 202).

    The inject_tinylora path (used by all training scripts) always uses the
    small random init. This is a reasonable choice (breaks symmetry), but
    differs from standard LoRA practice of zero-initializing B.

    Inconsistency: The two code paths disagree on initialization. If someone
    creates TinyLoRALinear directly without shared_v, they get pure zeros.

D6. RANDOM PROJECTION DISTRIBUTION
    Paper: "P_i in R^{r x r} are fixed random matrices" (no distribution specified)
    Code: Gaussian N(0, 1/r) via torch.randn / sqrt(frozen_rank)

    The 1/sqrt(r) scaling is standard variance normalization. Reasonable but
    unverifiable against the paper since the paper doesn't specify.

D7. NO LEARNING RATE SCHEDULE
    Paper mentions following SimpleRL settings for MATH training, which may
    include warmup. The code uses fixed LR with AdamW, no warmup or decay.

    For GSM8K-only experiments this may be acceptable (paper doesn't specify
    a schedule for GSM8K explicitly).

================================================================================
13. MINOR CONCERNS
================================================================================

M1. MERGE/UNMERGE BFLOAT16 PRECISION
    Merge: weight += delta * scale (in bfloat16)
    Unmerge: weight -= delta * scale (in bfloat16)
    Multiple cycles could accumulate floating-point drift. In practice, models
    are typically merged once, so this is unlikely to cause issues.

M2. torch.load WITHOUT weights_only
    Files checkpoint_manager.py, train_sft_tinylora.py, train_grpo_tinylora.py
    use torch.load(..., map_location="cpu") without weights_only=True.
    PyTorch >= 2.6 warns about this. Not a correctness issue but a security
    best practice for loading untrusted checkpoints.

M3. GENERATION DURING model.train()
    GRPO script calls model.generate() while model is in train() mode.
    For most causal LMs this is fine (no dropout during generation), but it's
    technically incorrect practice. Should call model.eval() for generation
    and model.train() for the backward pass.

M4. MEMORY EFFICIENCY OF GRPO
    The GRPO script does separate forward passes per sample within a group,
    accumulating computation graphs. For large group sizes this could be
    memory-intensive. A batched approach would be more efficient.

M5. grad_norm COMPUTED AFTER optimizer.step()
    In train_sft_tinylora.py:294, grad_norm is computed after optimizer.step().
    Some optimizers (like AdamW) may modify gradients during step. The logged
    grad_norm may not reflect the actual gradient used for the update. Should
    be computed before optimizer.step().

================================================================================
14. WHAT IS CORRECT
================================================================================

The following are verified correct against the paper:

  [✓] Core TinyLoRA update rule: W' = W + U Sigma (sum v_i P_i) V^T
  [✓] SVD decomposition: truncated SVD of frozen weights, rank clamped
  [✓] SVD factor shapes: U [out, r], S [r], V [in, r]
  [✓] Random projection tensor: P [u, r, r] per module, frozen buffer
  [✓] Trainable parameter: only v [u] per shared group
  [✓] Frozen rank default r=2 (paper ablation confirms r=2 is optimal)
  [✓] Target modules: 7 per layer (q/k/v/o/up/down/gate)
  [✓] Tiled tie-sharing: nearby modules share regardless of type
  [✓] Structured tie-sharing: same-type modules share
  [✓] Budget planner: correctly achieves 13 params for 224 modules
  [✓] Forward pass: efficient x @ V @ M^T @ diag(S) @ U^T (no full delta)
  [✓] Merge/unmerge: consistent with forward path
  [✓] Parameter deduplication: shared nn.Parameter counted once by PyTorch
  [✓] SFT prompt masking: labels=-100 for prompt tokens
  [✓] GSM8K answer extraction: #### <answer> regex with number fallback
  [✓] Exact-match reward: normalized comparison
  [✓] Deterministic seeding: reproducible P matrices and v initialization
  [✓] Device consistency: shared vectors on correct device, cross-device guard
  [✓] Dtype preservation: base weight dtype maintained through injection
  [✓] Checkpoint integrity: SHA256 manifests, atomic writes, archive policy

================================================================================
15. RECOMMENDATIONS
================================================================================

HIGH PRIORITY:
  1. Add a full-fidelity config that matches paper's GSM8K settings:
     epochs=3, group_size=4, batch_size=64, max_gen=4096, full dataset
  3. Document the GRPO simplification more precisely - explain what is and
     isn't implemented vs. Shao et al. 2024
  4. Add \boxed{} answer extraction for MATH/AIME/AMC evaluation

MODERATE:
  5. Document the alpha/projection_dim scaling choice and its absence from
     the paper formula
  6. Unify v initialization between TinyLoRALinear and inject_tinylora
  7. Move grad_norm computation before optimizer.step() in both training scripts
  8. Add weights_only=True to torch.load calls

LOW:
  9. Switch model to eval() during generation in GRPO script
 10. Consider batched forward passes in GRPO for memory efficiency

================================================================================
END OF AUDIT
================================================================================
